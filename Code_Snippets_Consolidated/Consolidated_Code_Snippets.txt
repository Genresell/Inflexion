import pandas as pd
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urlparse

import torch
from transformers import BertTokenizer, BertLMHeadModel

import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import numpy as np
import os
nltk.download("stopwords")
from nltk.corpus import stopwords
stopword=nltk.corpus.stopwords.words('english')
nltk.download("wordnet")
nltk.download('omw-1.4')
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
nltk.download('punkt')
#Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import coherencemodel
# Plotting tools
import pyLDAvis
import pyLDAvis.sklearn
import matplotlib.pyplot as plt
%matplotlib inline
# Sklearn
from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import GridSearchCV
from pprint import pprint
import re, nltk,gensim
import warnings
warnings.filterwarnings('ignore')

import schedule
import time

data = []

def web_scraper():
    urls = [
    'https://www.wsj.com/',
    'https://www.cnn.com/',
    'https://www.nytimes.com/',
    'https://www.theguardian.com/international',
    'https://www.reuters.com/news/world'
    ]

    # Initialize an empty list to store the scraped data
    articles = []

    for url in urls:
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        articles += soup.find_all('article')

    for article in articles:
        article_url = article.find('a')['href']
        if not article_url.startswith('http'):
            article_url = f'https://{urlparse(url).netloc}{article_url}'

        title = article.find('h3')
        if title:
            title = title.text.strip()

        date = article.find('time')
        if date:
            try:
                date = datetime.strptime(date['datetime'], '%Y-%m-%dT%H:%M:%S.%fZ')
                date = date.strftime('%Y-%m-%d %H:%M:%S')
            except ValueError:
                date = None

        author = article.find('span', class_='c-byline__name')
        if author:
            author = author.text.strip()

        content = ''
        content_url = article_url
        content_response = requests.get(content_url)
        content_soup = BeautifulSoup(content_response.content, 'html.parser')
        paragraphs = content_soup.find_all('p')
        for paragraph in paragraphs:
            content += paragraph.text

        data.append({
            'title': title,
            'date': date,
            'author': author,
            'content': content,
            'url': article_url
        })

web_scraper()

df = pd.DataFrame(data)
print(df.tail())

# load pre-trained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertLMHeadModel.from_pretrained('bert-base-uncased')

# define function to preprocess text data for BERT
def preprocess_text(text):
    # tokenize text
    tokenized_text = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        truncation=True,
        max_length=127,
        return_token_type_ids=False,
        padding='max_length',
        return_attention_mask=True,
        return_tensors='pt'
    )

    return tokenized_text

# define function to generate summary using BERT
def generate_summary(text):
    # preprocess text
    tokenized_text = preprocess_text(text)

    # generate summary
    summary_ids = model.generate(
        input_ids=tokenized_text['input_ids'],
        attention_mask=tokenized_text['attention_mask'],
        max_length=128,
        num_beams=4,
        early_stopping=True
    )

    # decode summary
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    return summary

# preprocess content and generate summary for each article in dataframe
df['content_preprocessed'] = df['content'].apply(preprocess_text)
df['summary'] = df['content'].apply(generate_summary)

# view final dataframe with summary column added
print(df.tail())

analyzer = SentimentIntensityAnalyzer()
df['sentiment_scores'] = df['summary'].apply(lambda x: analyzer.polarity_scores(x))

def get_sentiment_label(compound_score):
    if compound_score >= 0.05:
        return 'positive'
    elif compound_score <= -0.05:
        return 'negative'
    else:
        return 'neutral'
    
df['sentiment_label'] = df['sentiment_scores'].apply(lambda x: get_sentiment_label(x['compound']))

# view final dataframe with sentiment scores column added
print(df.tail())

# Define the sentiment analysis function
def analyze_sentiment(text):
    # Instantiate the SentimentIntensityAnalyzer from NLTK
    sia = SentimentIntensityAnalyzer()

    # Analyze the sentiment of the text
    sentiment = sia.polarity_scores(text)

    return sentiment

# Define the function to update the data analysis and insights
def update_insights(df):
    # Generate a summary for each article
    df["summary"] = df["content"].apply(generate_summary)

    # Analyze the sentiment of each article summary
    df["sentiment"] = df["summary"].apply(analyze_sentiment)

    # Generate insights based on the sentiment analysis
    positive_articles = df[df["sentiment"]["compound"] > 0.5]
    negative_articles = df[df["sentiment"]["compound"] < -0.5]
    neutral_articles = df[(df["sentiment"]["compound"] >= -0.5) & (df["sentiment"]["compound"] <= 0.5)]

    # Print the number of positive, negative, and neutral articles
    print("Number of positive articles: {}".format(len(positive_articles)))
    print("Number of negative articles: {}".format(len(negative_articles)))
    print("Number of neutral articles: {}".format(len(neutral_articles)))

    # Generate visualizations of the sentiment analysis results

import re 
def preprocess_text(text):
    text = re.sub('((www\.[^\s]+)|-(https?://[^\s]+))',' ', text)
    text = re.sub('@[^\s]+',' ', text)
    text = text.lower().replace("ё", "е")
    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)
    text = re.sub(r'\d+','', text)
    text = re.sub(r'\b\w{1,3}\b', '', text)
    return text.strip()

df["cleaned"]=df["summary"].apply(preprocess_text)

#Removing extra unnecessary words.
new_stopwords=['april', 'reuters', 'april reuters']

stopword.extend(new_stopwords)

#Remove stopword
df["cleaned"]=df["cleaned"].apply(lambda x: " ".join(x for x in x.split() if x not in stopword))
df["cleaned"]

#Normalization
lemma = WordNetLemmatizer()
df["cleaned"]=df["cleaned"].apply(lambda x: "".join(lemma.lemmatize(w) for w in x))
data_lemmatized=df["cleaned"]

vectorizer = CountVectorizer(analyzer='word',       
                             min_df=1,                        # minimum reqd occurences of a word 
                             stop_words='english',             # remove stop words
                             lowercase=True,                   # convert all words to lowercase
                             token_pattern='[a-zA-Z0-9]{3,}',
                             ngram_range=(1,3)  # num chars > 3
                             # max_features=50000,             # max number of uniq words
                            )

data_vectorized = vectorizer.fit_transform(data_lemmatized)

# Materialize the sparse data
data_dense = data_vectorized.todense()

# Compute Sparsicity = Percentage of Non-Zero cells
print("Sparsicity: ", ((data_dense > 0).sum()/data_dense.size)*100, "%")

# Build LDA Model
lda_model = LatentDirichletAllocation(n_components=20,               # Number of topics
                                      max_iter=10,               # Max learning iterations
                                      learning_method='batch',   
                                      random_state=100,          # Random state
                                      batch_size=128,            # n docs in each learning iter
                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't
                                      n_jobs = -1,               # Use all available CPUs
                                     )
lda_output = lda_model.fit_transform(data_vectorized)

# Log Likelyhood: Higher the better
print("Log Likelihood: ", lda_model.score(data_vectorized))

# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)
print("Perplexity: ", lda_model.perplexity(data_vectorized))

# See model parameters
print(lda_model.get_params())

# Define Search Param
search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}

# Init the Model
lda = LatentDirichletAllocation()

# Init Grid Search Class
model = GridSearchCV(lda, param_grid=search_params)

# Do the Grid Search
model.fit(data_vectorized)

# Best Model
best_lda_model = model.best_estimator_

# Model Parameters
print("Best Model's Params: ", model.best_params_)

# Log Likelihood Score
print("Best Log Likelihood Score: ", model.best_score_)

# Perplexity
print("Model Perplexity: ", best_lda_model.perplexity(data_vectorized))

# Get Log Likelyhoods from Grid Search Output
n_topics = [10, 15, 20, 25, 30]
log_likelyhoods_5 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.5]
log_likelyhoods_7 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.7]
log_likelyhoods_9 = [round(model.cv_results_['mean_test_score'][index]) for index, gscore in enumerate(model.cv_results_['params']) if gscore['learning_decay']==0.9]

# Show graph
plt.figure(figsize=(12, 8))
plt.plot(n_topics, log_likelyhoods_5, label='0.5')
plt.plot(n_topics, log_likelyhoods_7, label='0.7')
plt.plot(n_topics, log_likelyhoods_9, label='0.9')
plt.title("Choosing Optimal LDA Model")
plt.xlabel("Num Topics")
plt.ylabel("Log Likelyhood Scores")
plt.legend(title='Learning decay', loc='best')
plt.show()

# Create Document - Topic Matrix
lda_output = best_lda_model.transform(data_vectorized)

# column names
topicnames = ["Topic" + str(i) for i in range(best_lda_model.n_components)]

# index names
docnames = ["Doc" + str(i) for i in range(len(data))]

# Make the pandas dataframe
df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)

# Get dominant topic for each document
dominant_topic = np.argmax(df_document_topic.values, axis=1)
df_document_topic['dominant_topic'] = dominant_topic

# Styling
def color_green(val):
    color = 'green' if val > .1 else 'black'
    return 'color: {col}'.format(col=color)

def make_bold(val):
    weight = 700 if val > .1 else 400
    return 'font-weight: {weight}'.format(weight=weight)

# Apply Style
df_document_topics = df_document_topic.style.applymap(color_green).applymap(make_bold)
df_document_topics

df = df.set_index(df_document_topics.index)

df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name="Num Documents")
df_topic_distribution.columns = ['Topic Num', 'Num Documents']
df_topic_distribution

# Topic-Keyword Matrix
df_topic_keywords = pd.DataFrame(best_lda_model.components_)

# Assign Column and Index
df_topic_keywords.columns = vectorizer.get_feature_names_out()
df_topic_keywords.index = topicnames

# View
df_topic_keywords.head()

# Show top 15 keywords for each topic
def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):
    keywords = np.array(vectorizer.get_feature_names_out())
    topic_keywords = []
    
    for topic_weights in lda_model.components_:
        top_keyword_locs = (-topic_weights).argsort()[:n_words]
        topic_keywords.append(keywords.take(top_keyword_locs))
    return topic_keywords

topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)        

# Topic - Keywords Dataframe
df_topic_keywords = pd.DataFrame(topic_keywords)
df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]
df_topic_keywords["Topic"]= [str(i+1) for i in range(df_topic_keywords.shape[0])]
df_topic_keywords["keywords"]=topic_keywords
df_topic_keywords

df1=df_document_topics.data

df1.columns

df['Topics']=df1['dominant_topic']
df['Topics']

df.head()

df_topic_keywords['Topics']=df_topic_keywords['Topic']

df.rename(columns={"Final Topic":'Final_Topic'},inplace=True)
df_topic_keywords.rename(columns={"Final_Topic":'Final_Topic_1'},inplace=True)

df_topic_keywords['Topics']=df_topic_keywords['Topics'].astype(int)
df['Topics']=df['Topics'].astype(int)

data_2=pd.merge(df,df_topic_keywords[['Topics','keywords']],left_on = 'Topics',right_on= 'Topics',how='left')

data_2.tail()

# Define function to run the pipeline
def run_pipeline():
    # Run the web scraper
    process = CrawlerProcess(get_project_settings())
    process.crawl(MySpider)
    process.start()

    # Load the scraped data into a dataframe
    new_data = pd.read_csv('data.csv')

    # Generate summaries for the new data
    new_data['summary'] = new_data['text'].apply(summarize_text)

    # Combine the new data with the existing data
    all_data = pd.concat([data, new_data])

    # Perform sentiment analysis on the summaries
    vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)
    data_vectorized = vectorizer.fit_transform(all_data['summary'])
    lda_model = LatentDirichletAllocation(n_components=3, random_state=42)
    lda_model.fit(data_vectorized)
    lda_output = lda_model.transform(data_vectorized)
    all_data['sentiment_label'], all_data['sentiment_scores'] = zip(*all_data['summary'].apply(analyze_sentiment))

    # Visualize the topics and sentiment scores
    panel = pyLDAvis.sklearn.prepare(lda_model, data_vectorized, vectorizer, mds='tsne')
    pyLDAvis.save_html(panel, 'panel.html')
    print('Pipeline run complete.')

# Define the main function that runs the pipeline
def main():
    # Run the web scraper
    df = web_scraper()

    # Update the data analysis and insights
    update_insights(df)

# Use the schedule library to run the main function periodically
schedule.every(24).hours.do(main)

# Use a try-except block to handle any exceptions that occur during execution
try:
    while True:
        schedule.run_pending()
        time.sleep(1)
except Exception as e:
    print("An error occurred: {}".format(str(e)))